{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50571b6d-4578-48a7-8864-cfa7565dfcbc",
   "metadata": {},
   "source": [
    "This is a local, non-Kaggle notebook in which TFX 1.16.0 and python 3.10 and the compatible versions of other libraries are installed in a virtual environment that this notebook is running in.\n",
    "\n",
    "paths are relative to the github repository directory, \"recommender_systems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed99ee786e13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from tfx.orchestration import metadata\n",
    "\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from ml_metadata.metadata_store import metadata_store\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src/test/python/movie_lens_tfx\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src/main/python/movie_lens_tfx\"))\n",
    "\n",
    "from helper import *\n",
    "from movie_lens_tfx.PipelineComponentsFactory import *\n",
    "from movie_lens_tfx.tune_train_movie_lens import *\n",
    "\n",
    "from absl import logging\n",
    "tf.get_logger().propagate = False\n",
    "logging.set_verbosity(logging.WARNING)\n",
    "logging.set_stderrthreshold(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c98cdd-f021-49f6-af4a-aa571bf03b04",
   "metadata": {},
   "source": [
    "## EDA on the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad33acc-b994-420b-a0ad-ae1f178987e3",
   "metadata": {},
   "source": [
    "### w/ Polars, Seaborn, and Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf330e05-f116-43d0-b35d-8414f0fba1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "#import matplotlib.pyplot as plt\n",
    "#seaborn version installed is 0.12.2.  need>= 0.13.0 for polars\n",
    "#import seaborn as sns\n",
    "#import seaborn_polars as snl\n",
    "from scipy.stats.distributions import chi2\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import io\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import dcor\n",
    "import numpy as np\n",
    "#import altair as alt\n",
    "import plotly.express as px\n",
    "#needs pip install plotly jupyterlab anywidget\n",
    "\n",
    "pl.Config.set_fmt_str_lengths(900)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e6b19f-4bd3-46eb-b4a2-537f9f2507fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def can_reject_indep(x : np.array, y:np.array, alpha:float = 0.05, debug:bool=False):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    x: float array\n",
    "    y: float array\n",
    "  reject independence for \n",
    "    n*C >= inv(F{chi^2-1})(1-alpha)\n",
    "    where n = len(x)\n",
    "      C = fast distance covariance following 2019 Chaudhuri and Hu\n",
    "      inv(F{chi^2-1}) is the inverse of the CDF.\n",
    "  \"\"\"\n",
    "  with np.errstate(divide='ignore'):\n",
    "    C = dcor.distance_covariance(x, y, method='mergesort')\n",
    "  lhs = len(x)*C\n",
    "  rhs = chi2.ppf(1 - alpha, df=x.shape[-1])\n",
    "  if debug:\n",
    "    print(f\"nC={lhs}\\nppf(1-{alpha}, dof={x.shape[-1]})={rhs}\")\n",
    "  return lhs >= rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb3dc9b-3c93-4830-b30e-d6cd884f6634",
   "metadata": {},
   "outputs": [],
   "source": [
    "CTZ = pytz.timezone(\"America/Chicago\")\n",
    "genres = [\"Action\", \"Adventure\", \"Animation\", \"Children\", \"Comedy\",\n",
    "          \"Crime\", \"Documentary\", \"Drama\", \"Fantasy\", \"Film-Noir\",\n",
    "          \"Horror\", \"Musical\", \"Mystery\", \"Romance\", \"Sci-Fi\",\n",
    "          \"Thriller\", \"War\", \"Western\"]\n",
    "\n",
    "schemas = {\n",
    "  'ratings' : pl.Schema(OrderedDict({'user_id': pl.Int64, \n",
    "    'movie_id': pl.Int64, 'rating': pl.Int64,\n",
    "    'timestamp' : pl.Int64})),\n",
    "  'users' : pl.Schema(OrderedDict({'user_id': pl.Int64, \n",
    "    'gender': pl.String, 'age': pl.Int64,\n",
    "    'occupation' : pl.Int64, \n",
    "    'zipcode' : pl.String})),\n",
    "  'movies' : pl.Schema(OrderedDict({'movie_id': pl.Int64, \n",
    "    'title': pl.String, 'genres': pl.String}))}\n",
    "\n",
    "_infiles_dict_ser, _, __ = get_test_data(use_small=False)\n",
    "_infiles_dict = deserialize(_infiles_dict_ser)\n",
    "\n",
    "file_paths = {\n",
    "  'ratings': _infiles_dict['ratings']['uri'],\n",
    "  'users':_infiles_dict['users']['uri'],\n",
    "  'movies':_infiles_dict['movies']['uri'],\n",
    "}\n",
    "\n",
    "#polars.read_csv( source=\n",
    "#  encoding='iso-8859-1', \n",
    "#  has_header=False, skip_rows=0, try_parse_dates=True, \n",
    "#  use_pyarrow=True\n",
    "\n",
    "labels_dict = {}\n",
    "labels_dict['age_group'] = {0:'1', 1:'18', 2:'25', 3:'35', 4:'45', 5:'50', 6:'56'} \n",
    "labels_dict['gender'] = {0:'F', 1:'M'}\n",
    "labels_dict['occupation'] = {0:  \"other\", 1:  \"academic/educator\", 2:  \"artist\",\n",
    "    3:  \"clerical/admin\", 4:  \"college/grad student\", 5:  \"customer service\",\n",
    "    6:  \"doctor/health care\", 7:  \"executive/managerial\", 8:  \"farmer\", 9:  \"homemaker\",\n",
    "    10:  \"K-12 student\", 11:  \"lawyer\", 12:  \"programmer\", 13:  \"retired\",\n",
    "    14:  \"sales/marketing\", 15:  \"scientist\", 16:  \"self-employed\", 17:  \"technician/engineer\",\n",
    "    18:  \"tradesman/craftsman\", 19:  \"unemployed\", 20:  \"writer\"}\n",
    "labels_dict_arrays = {}\n",
    "for k in labels_dict:\n",
    "    labels_dict_arrays[k]=[labels_dict[k][k2] for k2 in labels_dict[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38fb215-9357-492d-b35b-3cfc1558d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving images to files to conserve memory\n",
    "!pip install -U -q kaleido\n",
    "img_dir = os.path.join(get_bin_dir(), \"local_notebook\", \"images\")\n",
    "os.makedirs(img_dir, exist_ok=True)\n",
    "\n",
    "for key in file_paths:\n",
    "    processed_buffer = io.StringIO()\n",
    "    file_path = file_paths[key]\n",
    "    schema = schemas[key]\n",
    "    print(f\"key={key}, file_path={file_path}\")\n",
    "    with open(file_path, \"r\", encoding='iso-8859-1') as file:\n",
    "        for line in file:\n",
    "            line2 = line.replace('::', '\\t')\n",
    "            processed_buffer.write(line2)\n",
    "\n",
    "    processed_buffer.seek(0)\n",
    "    df = pl.read_csv(processed_buffer,\n",
    "        encoding='iso-8859-1', has_header=False,\n",
    "        skip_rows=0, separator='\\t', schema=schema,\n",
    "        try_parse_dates=True,\n",
    "        new_columns=schema.names(),\n",
    "        use_pyarrow=True)\n",
    "\n",
    "    if key==\"movies\":\n",
    "        df = df.with_columns(\n",
    "          pl.col(\"genres\").str.replace(\"Children's\", \"Children\")\n",
    "        )\n",
    "        df = df.with_columns(\n",
    "          pl.col(\"genres\").str.split(\"|\")\n",
    "        )\n",
    "        movie_genres = df.explode('genres')\n",
    "        ordered_genres = movie_genres['genres'].value_counts().sort('count', descending=True)\n",
    "        fig = px.bar(ordered_genres, x=\"genres\", y=\"count\", title=\"genres histogram\",)\n",
    "        fig.write_image(os.path.join(img_dir, \"genres_hist.png\"))\n",
    "    if key==\"ratings\":\n",
    "        #user_id, movie_id, rating, timestamp\n",
    "        fig = px.histogram(df, x='rating', title='rating')\n",
    "        fig.write_image(os.path.join(img_dir, \"rating_hist.png\"))\n",
    "        fig = px.histogram(df, x='timestamp', title='timestamp')\n",
    "        fig.write_image(os.path.join(img_dir, \"timestamp_hist.png\"))\n",
    "        fig = px.histogram(df, x='movie_id', title='movie_id')\n",
    "        fig.write_image(os.path.join(img_dir, \"movieid_hist.png\"))\n",
    "        fig = px.histogram(df,  x='user_id', title='user_id')\n",
    "        fig.write_image(os.path.join(img_dir, \"userid_hist.png\"))\n",
    "        #run the ndep tests on transformed data instead of raw data\n",
    "        #x = df.select(pl.col(\"rating\")).to_numpy()\n",
    "        #y = df.select(pl.col(\"timestamp\")).to_numpy()\n",
    "        #print(f\"rating, timestamp are indep: {can_reject_indep(x, y, 0.05, True)}\")\n",
    "        fig = px.density_heatmap(df, x='movie_id', y='rating')\n",
    "        fig.write_image(os.path.join(img_dir, \"movieid_rating_heatmap.png\"))\n",
    "        fig = px.density_heatmap(df, x='timestamp', y='rating')\n",
    "        fig.write_image(os.path.join(img_dir, \"timestamp_rating_heatmap.png\"))\n",
    "        fig = px.density_heatmap(df, x='user_id', y='rating')\n",
    "        fig.write_image(os.path.join(img_dir, \"userid_rating_heatmap.png\"))\n",
    "        fig = px.density_heatmap(df, x='timestamp', y='movie_id')\n",
    "        fig.write_image(os.path.join(img_dir, \"timestamp_movieid_heatmap.png\"))\n",
    "        #fig = px.scatter_ternary(df, a=\"rating\", b=\"timestamp\", c=\"movie_id\",\n",
    "        #    #size=\"total\", size_max=15,\n",
    "        #    color_discrete_map = {\"rating\": \"blue\", \"timestamp\": \"green\", \"movie_id\":\"red\"} )\n",
    "        #fig.show(renderer='notebook')\n",
    "        #fig = px.scatter_ternary(df, a=\"rating\", b=\"user_id\", c=\"movie_id\",\n",
    "        #    #size=\"total\", size_max=15,\n",
    "        #    color_discrete_map = {\"rating\": \"blue\", \"user_id\": \"green\", \"movie_id\":\"red\"} )\n",
    "        #fig.show(renderer='notebook')\n",
    "    if key==\"users\":\n",
    "        #user_id, gender, age, occupation, zipcode\n",
    "        fig = px.histogram(df, x='gender', title='gender')\n",
    "        fig.write_image(os.path.join(img_dir, \"gender_hist.png\"))\n",
    "        fig = px.histogram(df, x='age',  title='age')\n",
    "        fig.write_image(os.path.join(img_dir, \"age_hist.png\"))\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"occupation\").map_elements(lambda x: labels_dict['occupation'].get(x,x)).alias(\"occ\")\n",
    "        )\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"occ\").cast(pl.Categorical)\n",
    "        )\n",
    "        ordered_occupation = df['occ'].value_counts().sort('count', descending=True)\n",
    "        fig = px.bar(ordered_occupation, x=\"occ\", y=\"count\", title=\"occupation histogram\",)\n",
    "        fig.update_xaxes(tickangle=45)\n",
    "        fig.write_image(os.path.join(img_dir, \"occupation_hist.png\"))\n",
    "        fig = px.histogram(df, x='zipcode',  title='zipcode')\n",
    "        fig.write_image(os.path.join(img_dir, \"zipcode_hist.png\"))\n",
    "        #run the ndep tests on transformed data instead of rrrawawwa data\n",
    "        _features=['gender', 'age', 'occupation', 'zipcode']\n",
    "        for ii, feature in enumerate(_features):\n",
    "            for jj in range(ii+1, len(_features)):\n",
    "                feature2 = _features[jj]\n",
    "                if feature2 == 'occupation':\n",
    "                    occ_counts = df.group_by(\"occ\").len().rename({\"len\": \"occ_count\"})\n",
    "                    df_sorted = (\n",
    "                        df.join(occ_counts, on=\"occ\", how=\"left\")\n",
    "                        .sort(by=\"occ_count\", descending=True)\n",
    "                        .drop(\"occ_count\")\n",
    "                    )\n",
    "                    fig = px.density_heatmap(df_sorted, x=feature, y=feature2)\n",
    "                else:\n",
    "                    fig = px.density_heatmap(df, x=feature, y=feature2)\n",
    "                fig.write_image(os.path.join(img_dir, f\"{feature}_{feature2}_heatmap.png\"))\n",
    "                #for kk in range(jj+1, len(_features)):\n",
    "                #    feature3 = _features[kk]\n",
    "                #    fig = px.scatter_ternary(df, a=feature, b=feature2, c=feature3,\n",
    "                #        #size=\"total\", size_max=15,\n",
    "                #        color_discrete_map = {feature: \"blue\", feature2: \"green\", feature3:\"red\"} )\n",
    "                #    fig.show(renderer='notebook')\n",
    "\n",
    "del df\n",
    "print(f'wrote univariate EDA images to {img_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6b2d4-2479-4069-aee1-59ecd131e040",
   "metadata": {},
   "source": [
    "### run data pre-processing on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693174b-6f67-48b0-a227-85324cb3bf2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infiles_dict_ser, output_config_ser, split_names = get_test_data(use_small=False)\n",
    "user_id_max = 6040\n",
    "movie_id_max = 3952\n",
    "n_genres = N_GENRES\n",
    "n_age_groups = N_AGE_GROUPS\n",
    "n_occupations = 21\n",
    "MIN_EVAL_SIZE = 50 #make this larger for production pipeline\n",
    "\n",
    "test_num = \"1\"\n",
    "    \n",
    "PIPELINE_NAME = 'TestPipelines'\n",
    "output_data_dir = os.path.join(get_bin_dir(), \"local_notebook\", test_num)\n",
    "PIPELINE_ROOT = os.path.join(output_data_dir, PIPELINE_NAME)\n",
    "\n",
    "# remove results from previous test runs:\n",
    "try:\n",
    "  print(f\"removing: {PIPELINE_ROOT}\")\n",
    "  shutil.rmtree(PIPELINE_ROOT)\n",
    "except OSError as e:\n",
    "  pass\n",
    "METADATA_PATH = os.path.join(PIPELINE_ROOT, 'tfx_metadata',\n",
    "                             'metadata.db')\n",
    "os.makedirs(os.path.join(PIPELINE_ROOT, 'tfx_metadata'),\n",
    "            exist_ok=True)\n",
    "\n",
    "ENABLE_CACHE = True\n",
    "\n",
    "# metadata_connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "# metadata_connection_config.sqlite.SetInParent()\n",
    "# metadata_connection = metadata.Metadata(metadata_connection_config)\n",
    "metadata_connection_config = metadata.sqlite_metadata_connection_config(\n",
    "  METADATA_PATH)\n",
    "\n",
    "store = metadata_store.MetadataStore(metadata_connection_config)\n",
    "\n",
    "if get_kaggle():\n",
    "  tr_dir = \"/kaggle/working/\"\n",
    "else:\n",
    "  tr_dir = os.path.join(get_project_dir(), \"src/main/python/movie_lens_tfx\")\n",
    "\n",
    "serving_model_dir = os.path.join(PIPELINE_ROOT, 'serving_model')\n",
    "output_parquet_path = os.path.join(PIPELINE_ROOT, \"transformed_parquet\")\n",
    "\n",
    "# for the custom ingestion component, the apache beam pipeline needs to be able to\n",
    "# find the sibling scripts it imports.\n",
    "# 2 solutions: (1) create a tar archive and use --extra_package in pipeline args\n",
    "# or (2) use setup.py and --setup_file in pipeline args.\n",
    "\n",
    "beam_pipeline_args = [\n",
    "  '--direct_running_mode=multi_processing',\n",
    "  '--direct_num_workers=0',\n",
    "  '--setup_file=setup.py',\n",
    "  #f'--extra_package={ingest_tar_file}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba56d14-a868-4878-a64f-74ade8c57b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "context = InteractiveContext(pipeline_name=PIPELINE_NAME, pipeline_root=PIPELINE_ROOT,\n",
    "  metadata_connection_config=metadata_connection_config,\n",
    "  beam_pipeline_args=beam_pipeline_args\n",
    ")\n",
    "\n",
    "factory = PipelineComponentsFactory(infiles_dict_ser, output_config_ser, tr_dir,\n",
    "    user_id_max, movie_id_max, n_genres, n_age_groups,\n",
    "    MIN_EVAL_SIZE, serving_model_dir, output_parquet_path)\n",
    "\n",
    "components = factory.build_components(PIPELINE_TYPE.PREPROCESSING)\n",
    "\n",
    "for component in components:\n",
    "    context.run(component)\n",
    "\n",
    "print(f'done pre-processing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dfcaa8-d03f-4c52-a47e-c32da31b6071",
   "metadata": {},
   "source": [
    "## EDA on the transformed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c52b9d-16d5-432c-acbc-b8d0e51b039d",
   "metadata": {},
   "source": [
    "### using Polars and Plotly.express "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9698562e-ccc3-4934-8b80-6dfa4dd7c0e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parquet_path = os.path.join(PIPELINE_ROOT, \"transformed_parquet\")\n",
    "from movie_lens_tfx.utils import movie_lens_utils\n",
    "\n",
    "for split_name in [\"train\", \"eval\", \"test\"]:\n",
    "    in_file_pattern = os.path.join(parquet_path, f\"Split-{split_name}*\")\n",
    "    df = pl.read_parquet(in_file_pattern)\n",
    "    #df = pl.scan_parquet(in_file_pattern)\n",
    "    df = df.with_columns(\n",
    "        pl.col(\"genres\").map_elements(movie_lens_utils.deserialize, return_dtype=pl.Object)\n",
    "    )\n",
    "    print(f\"{split_name}: {df.head(5)}\")\n",
    "    dist_corr_matrix = []\n",
    "    labels = []\n",
    "    for i, feature in enumerate(df.columns):\n",
    "        if feature == 'genres':\n",
    "            continue\n",
    "        labels.append(feature)\n",
    "        d = []\n",
    "        for j in range(i+1, len(df.columns)):\n",
    "            feature2 = df.columns[j]\n",
    "            if feature2 == 'genres':\n",
    "                continue\n",
    "            d.append(dcor.distance_correlation(df[feature], df[feature2], method='mergesort')\n",
    "        dist_corr_matrix.append(d)\n",
    "    fig = px.imshow(\n",
    "        dist_corr_matrix, zmin=0., zmax=1., labels=labels,\n",
    "        color_continuous_scale=\"RdBu_r\", # Red-Blue reversed for correlation\n",
    "        title=\"Correlation Matrix Heatmap\"\n",
    "    )\n",
    "    fig.write_image(os.path.join(img_dir, f\"{split_name}_distcorr_heatmap.png\"))\n",
    "\n",
    "    \"\"\"\n",
    "    explode each genres into new columns by name\n",
    "    \n",
    "    bar plots to compare avg feature with each genre\n",
    "    \n",
    "    correlation between genres, shown as heatmap to see which are frequently grouped together\n",
    "\n",
    "    pairplots of rating, gender, age, occupation, hr_wk, month, weekday\n",
    "\n",
    "    pairplots of feature with each genre\n",
    "\n",
    "    boxplot(data=data, x='genres', y='rating', hue='gender', palette='pastel')\n",
    "    boxplot(data=data, x='genres', y='rating', hue='age', palette='pastel')\n",
    "    violinplots instead\n",
    "\n",
    "    piechart of movie genres with slice thickness representing numbers rated\n",
    "    same pie chart for ratings=4.0 (== tranformed 1.)\n",
    "    same for ratings=3.0 ...\n",
    "\n",
    "    many more to add...\n",
    "    \n",
    "    consider a quick PCA\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    'user_id', 'movie_id'\n",
    "    'rating',\n",
    "    'gender', 'age', 'occupation', 'genres', 'hr', 'weekday', 'hr_wk','month'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c493f-6964-452d-997d-e51cb246c912",
   "metadata": {},
   "source": [
    "### using TFDV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2830d29-5eba-4978-bcd2-eec4743b6c4c",
   "metadata": {},
   "source": [
    "#load the transformed examples\n",
    "\n",
    "from tfx.dsl.io import fileio\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.components import StatisticsGen, SchemaGen, ExampleValidator\n",
    "from tfx.utils import io_utils\n",
    "from tensorflow_metadata.proto.v0 import anomalies_pb2, schema_pb2\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "\n",
    "#from movie_lens_tfx.ingest_pyfunc_component.ingest_movie_lens_component import *\n",
    "#from movie_lens_tfx.tune_train_movie_lens import *\n",
    "#from tfx import v1 as tfx\n",
    "\n",
    "schema_list = store.get_artifacts_by_type(\"Schema\")\n",
    "schema_list = sorted(schema_list,\n",
    "  key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in schema_list:\n",
    "    if \"post_transform_schema\" in artifact.uri:\n",
    "        schema_uri = artifact.uri\n",
    "        break\n",
    "assert(schema_uri is not None)\n",
    "schema_file_path = [os.path.join(schema_uri, name) for name in os.listdir(schema_uri)][0]\n",
    "schema = tfx.utils.parse_pbtxt_file(schema_file_path, schema_pb2.Schema())\n",
    "feature_spec = schema_utils.schema_as_feature_spec(schema).feature_spec\n",
    "\n",
    "examples_list = store.get_artifacts_by_type(\"Examples\")\n",
    "#print(f\"examples_list={examples_list}\")\n",
    "examples_list = sorted(examples_list,\n",
    "  key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in examples_list:\n",
    "    if \"transformed_examples\" in artifact.uri:\n",
    "        transformed_examples_uri = artifact.uri\n",
    "        break\n",
    "assert(transformed_examples_uri is not None)\n",
    "logging.debug(f\"transfomed_examples_uri={transformed_examples_uri}\")\n",
    "transform_uri = transformed_examples_uri[0:transformed_examples_uri.index(\"transformed_examples\")]\n",
    "\n",
    "\"\"\"\n",
    "transformed_examples\n",
    "post_transform_anomalies \n",
    "post_transform_schema\n",
    "pre_transform_stats\n",
    "post_transform_stats\n",
    "transform_graph\n",
    "updated_analyzer_cache\n",
    "pre_transform_schema\n",
    "\"\"\"\n",
    "\n",
    "def parse_tf_example(example_proto, feature_spec):\n",
    "    return tf.io.parse_single_example(example_proto, feature_spec)\n",
    "for split_name in [\"train\", \"eval\", \"test\"]:\n",
    "    tfrecord_uri = os.path.join(transform_uri, f\"Split-{split_name}\")\n",
    "    file_paths = [os.path.join(tfrecord_uri, name) for name in os.listdir(tfrecord_uri)]\n",
    "    ds_ser = tf.data.TFRecordDataset(file_paths, compression_type=\"GZIP\")\n",
    "    ds = ds_ser.map(lambda x: parse_tf_example(x, feature_spec))\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307091b-211e-4f81-95ae-eed7ef9389b2",
   "metadata": {},
   "source": [
    "## Run baseline model pipeline with full dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2ef186-8c42-4e25-a8ca-cc0295879a68",
   "metadata": {},
   "source": [
    "pipeline_factory = PipelineComponentsFactory(\n",
    "  infiles_dict_ser=infiles_dict_ser, output_config_ser=output_config_ser,\n",
    "  transform_dir=tr_dir, user_id_max=user_id_max, movie_id_max=movie_id_max,\n",
    "  n_genres=n_genres, n_age_groups=n_age_groups, min_eval_size=MIN_EVAL_SIZE,\n",
    "  serving_model_dir=serving_model_dir,\n",
    ")\n",
    "\n",
    "beam_pipeline_args = [\n",
    "  '--direct_running_mode=multi_processing',\n",
    "  '--direct_num_workers=0'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1ddd2-250f-4ebb-be31-dac45c7845ea",
   "metadata": {},
   "source": [
    "baseline_components = pipeline_factory.build_components(MODEL_TYPE.BASELINE)\n",
    "    \n",
    "# create baseline model\n",
    "my_pipeline = tfx.dsl.Pipeline(\n",
    "  pipeline_name=PIPELINE_NAME,\n",
    "  pipeline_root=PIPELINE_ROOT,\n",
    "  components=baseline_components,\n",
    "  enable_cache=ENABLE_CACHE,\n",
    "  metadata_connection_config=metadata_connection_config,\n",
    "  beam_pipeline_args=beam_pipeline_args,\n",
    ")\n",
    "\n",
    "tfx.orchestration.LocalDagRunner().run(my_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e2c108-fea6-4d88-a788-d4c21ebfd0a1",
   "metadata": {},
   "source": [
    "artifact_types = store.get_artifact_types()\n",
    "logging.debug(f\"MLMD store artifact_types={artifact_types}\")\n",
    "artifacts = store.get_artifacts()\n",
    "logging.debug(f\"MLMD store artifacts={artifacts}\")\n",
    "\n",
    "components = pipeline_factory.build_components(MODEL_TYPE.PRODUCTION)\n",
    "# simulate experimentation of one model family\n",
    "my_pipeline = tfx.dsl.Pipeline(\n",
    "  pipeline_name=PIPELINE_NAME,\n",
    "  pipeline_root=PIPELINE_ROOT,\n",
    "  components=components,\n",
    "  enable_cache=ENABLE_CACHE,\n",
    "  metadata_connection_config=metadata_connection_config,\n",
    "  beam_pipeline_args=beam_pipeline_args,\n",
    ")\n",
    "\n",
    "tfx.orchestration.LocalDagRunner().run(my_pipeline)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4cee9-c91e-49e3-a0ec-a6389f1a01c5",
   "metadata": {},
   "source": [
    "artifact_types = store.get_artifact_types()\n",
    "print(f\"MLMD store artifact_types={artifact_types}\")\n",
    "artifacts = store.get_artifacts()\n",
    "print(f\"MLMD store artifacts={artifacts}\")\n",
    "\n",
    "executions = store.get_executions()\n",
    "logging.debug(f\"MLMD store executions={executions}\")\n",
    "\n",
    "# executions has custom_properties.key: \"infiles_dict_ser\"\n",
    "#    and custom_properties.key: \"output_config_ser\"\n",
    "artifact_count = len(artifacts)\n",
    "execution_count = len(executions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
