{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50571b6d-4578-48a7-8864-cfa7565dfcbc",
   "metadata": {},
   "source": [
    "This is a local, non-Kaggle notebook in which TFX 1.16.0 and python 3.10 and the compatible versions of other libraries are installed in a virtual environment that this notebook is running in.\n",
    "\n",
    "paths are relative to the github repository directory, \"recommender_systems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed99ee786e13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "from tfx.orchestration import metadata\n",
    "\n",
    "import tensorflow_transform as tft\n",
    "\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "from ml_metadata.metadata_store import metadata_store\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src/test/python/movie_lens_tfx\"))\n",
    "sys.path.append(os.path.join(os.getcwd(), \"src/main/python/movie_lens_tfx\"))\n",
    "\n",
    "from helper import *\n",
    "from movie_lens_tfx.PipelineComponentsFactory import *\n",
    "from movie_lens_tfx.tune_train_movie_lens import *\n",
    "\n",
    "from absl import logging\n",
    "tf.get_logger().propagate = False\n",
    "logging.set_verbosity(logging.WARNING)\n",
    "logging.set_stderrthreshold(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c98cdd-f021-49f6-af4a-aa571bf03b04",
   "metadata": {},
   "source": [
    "## EDA on the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad33acc-b994-420b-a0ad-ae1f178987e3",
   "metadata": {},
   "source": [
    "### w/ Polars and Plotly express\n",
    "output is written to bin/local_notebook/images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95770000-3547-4541-8657-8b51fa0aa731",
   "metadata": {},
   "source": [
    "%run src/main/python/eda/eda_raw.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fec77f-0192-4d93-b922-5d5c3be21383",
   "metadata": {},
   "source": [
    "the generated images aren't plotted here, but similar plots are shown via TFDV below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6b2d4-2479-4069-aee1-59ecd131e040",
   "metadata": {},
   "source": [
    "### Run data pre-processing on full dataset to get the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693174b-6f67-48b0-a227-85324cb3bf2e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "infiles_dict_ser, output_config_ser, split_names = get_test_data(use_small=False)\n",
    "user_id_max = 6040\n",
    "movie_id_max = 3952\n",
    "n_genres = N_GENRES\n",
    "n_age_groups = N_AGE_GROUPS\n",
    "n_occupations = 21\n",
    "MIN_EVAL_SIZE = 50 #make this larger for production pipeline\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "test_num = \"1\"\n",
    "    \n",
    "PIPELINE_NAME = 'TestPipelines'\n",
    "output_data_dir = os.path.join(get_bin_dir(), \"local_notebook\", test_num)\n",
    "PIPELINE_ROOT = os.path.join(output_data_dir, PIPELINE_NAME)\n",
    "\n",
    "# remove results from previous test runs:\n",
    "try:\n",
    "  print(f\"removing: {PIPELINE_ROOT}\")\n",
    "  shutil.rmtree(PIPELINE_ROOT)\n",
    "except OSError as e:\n",
    "  pass\n",
    "METADATA_PATH = os.path.join(PIPELINE_ROOT, 'tfx_metadata',\n",
    "                             'metadata.db')\n",
    "os.makedirs(os.path.join(PIPELINE_ROOT, 'tfx_metadata'),\n",
    "            exist_ok=True)\n",
    "\n",
    "ENABLE_CACHE = True\n",
    "\n",
    "# metadata_connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "# metadata_connection_config.sqlite.SetInParent()\n",
    "# metadata_connection = metadata.Metadata(metadata_connection_config)\n",
    "metadata_connection_config = metadata.sqlite_metadata_connection_config(\n",
    "  METADATA_PATH)\n",
    "\n",
    "store = metadata_store.MetadataStore(metadata_connection_config)\n",
    "\n",
    "if get_kaggle():\n",
    "  tr_dir = \"/kaggle/working/\"\n",
    "else:\n",
    "  tr_dir = os.path.join(get_project_dir(), \"src/main/python/movie_lens_tfx\")\n",
    "\n",
    "serving_model_dir = os.path.join(PIPELINE_ROOT, 'serving_model')\n",
    "output_parquet_path = os.path.join(PIPELINE_ROOT, \"transformed_parquet\")\n",
    "\n",
    "# for the custom ingestion component, the apache beam pipeline needs to be able to\n",
    "# find the sibling scripts it imports.\n",
    "# 2 solutions: (1) create a tar archive and use --extra_package in pipeline args\n",
    "# or (2) use setup.py and --setup_file in pipeline args.\n",
    "\n",
    "beam_pipeline_args = [\n",
    "  '--direct_running_mode=multi_processing',\n",
    "  '--direct_num_workers=0',\n",
    "  '--setup_file=setup.py',\n",
    "  #f'--extra_package={ingest_tar_file}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a4011-c7cf-4ed2-9bb7-5f3a53af12d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "\n",
    "tf.get_logger().propagate = False\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.WARNING)\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.WARNING)\n",
    "logging.set_stderrthreshold(logging.WARNING)\n",
    "\n",
    "context = InteractiveContext(pipeline_name=PIPELINE_NAME, pipeline_root=PIPELINE_ROOT,\n",
    "  metadata_connection_config=metadata_connection_config,\n",
    "  beam_pipeline_args=beam_pipeline_args\n",
    ")\n",
    "\n",
    "factory = PipelineComponentsFactory(num_examples=1000209, infiles_dict_ser, output_config_ser, tr_dir,\n",
    "    user_id_max, movie_id_max, n_genres, n_age_groups,\n",
    "    MIN_EVAL_SIZE, batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, device=\"CPU\",\n",
    "    serving_model_dir, output_parquet_path)\n",
    "\n",
    "components = factory.build_components(PIPELINE_TYPE.PREPROCESSING)\n",
    "\n",
    "for component in components:\n",
    "    context.run(component)\n",
    "\n",
    "print(f'done pre-processing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668d235-1d07-439b-a6c3-beaa701f3315",
   "metadata": {},
   "source": [
    "## EDA on the transformed data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc80d679-b7ab-40f4-90a3-77c24f06fd2e",
   "metadata": {},
   "source": [
    "### using Polars, Plotly.express \n",
    "\n",
    "this can take an hour on a single COTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b01259-457b-4c57-8a67-26b13ddd29ff",
   "metadata": {},
   "source": [
    "%run src/main/python/eda/eda_transformed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b6dec-5ee5-4ebe-bb37-d559f8438b21",
   "metadata": {},
   "source": [
    "Some of the images are shown here from a former saved run.\n",
    "\n",
    "You can see a clear correlation between rating and movie_id, then less\n",
    "so between rating and age and rating and occupation.\n",
    "\n",
    "<img src=\"src/test/resources/train_dist_corr_heatmap.png\">\n",
    "\n",
    "You can see many cooccurences between movie genres.\n",
    "\n",
    "<img src=\"src/test/resources/train_genre_cooccurence_rating_5_heatmap.png\">\n",
    "\n",
    "You can see that Drama, then Comedy, then Action are the most popular movie genres.\n",
    "\n",
    "<img src=\"src/test/resources/train_genres_rating_5.png\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f330e36e-74b9-4695-973c-25431bc3ea12",
   "metadata": {},
   "source": [
    "### using Pandas, Pyspark MLLIB FPGrowth\n",
    "\n",
    "This does a market basket analysis with movie_ids.\n",
    "\n",
    "If you want the PrefixSpan plots also, set\n",
    "PLOT_PREFIXSPAN = True\n",
    "but beware that the script will take much longer to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba2cda2-d3c6-4289-90a8-3ebc634373aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_PREFIXSPAN=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290d54c1-d223-40eb-9590-8e158f07a5aa",
   "metadata": {},
   "source": [
    "%run src/main/python/eda/eda_transformed_pyspark_mllib.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79c5727-401a-40a2-9ee1-145e000c266a",
   "metadata": {},
   "source": [
    "Some of the images are shown here from a former saved run.\n",
    "\n",
    "You can see that many association rules can be derived from the data.\n",
    "<img src=\"src/test/resources/train_movies_assoc_rules_rating_5.png\">\n",
    "\n",
    "and that there are many frequent itemsets.\n",
    "<img src=\"src/test/resources/train_movies_itemsets_rating_5.png\">\n",
    "\n",
    "<img src=\"src/test/resources/train_movies_itemsets_rating_5_2.png\">\n",
    "\n",
    "The results show that neural network models will have good material to exploit.\n",
    "\n",
    "The PrefixScan model can be enabled in the script to provide images to explore sequences\n",
    "for actions or as precursor to Sequential DNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c493f-6964-452d-997d-e51cb246c912",
   "metadata": {},
   "source": [
    "### Data and Concept Drift\n",
    "After exploring the data inputs to the model, we want to define monitoring\n",
    "for data and concept shifts.\n",
    "\n",
    "let X = features\n",
    "\n",
    "let Y = targets\n",
    "\n",
    "Data shift is a change in the joint distribution, P(X, Y). \n",
    "\n",
    "Using the probability product rule, we can explore 4 causes for the\n",
    "simplest changes in P(X,Y)\n",
    "$$ P(X, Y) = P(Y, X) = P(X|Y)P(Y) = P(Y|X)P(X) $$\n",
    "\n",
    "We can look for changes in one member in the following pairs at any time\n",
    "(one is simpler than exploring more than 1 member changing at same time):\n",
    "$$ P(X|Y) * P(Y) $$\n",
    "$$ P(Y|X) * P(X) $$\n",
    "\n",
    "* Covariate shift:\n",
    "  $$ P(X) changed.  P(Y|X) unchanged $$\n",
    "  Distr of model inputs changes.\n",
    "* Label shift:\n",
    "  $$ P(Y) changed,  P(X|Y) unchanged $$\n",
    "  Distr of model outputs changes, but for any given output, the input distribution stays the same.\n",
    "  \n",
    "* Concept shift:\n",
    "  $$ P(X) unchanged,  P(Y|X) changed $$\n",
    "* Manifestation shift:\n",
    "  $$ P(Y) unchanged,  P(X|Y) changed $$\n",
    "\n",
    "[see more at NannyML](https://www.nannyml.com/blog/concept-drift)\n",
    "\n",
    "if the train, eval, and test split are random, then one could use\n",
    "the maximum differences in their distributions as a lower limit on \n",
    "the estimate for stochastic error.  A trigger for data drift should\n",
    "then be GEQ about 3 times that stochastic error.\n",
    "\n",
    "The previous data doesn't exist yet for this project, but one could\n",
    "either download another movie-lens dataset of different time period,\n",
    "or split this 1M dataset by a timestamp ordering into 2 partitions.\n",
    "\n",
    "More data is available at [GroupLens](https://files.grouplens.org/datasets/movielens/)\n",
    "\n",
    "### Data [Drift and Skew using TFDV](https://www.tensorflow.org/tfx/tutorials/data_validation/tfdv_basic)\n",
    "* Drift\n",
    "  Drift detection is supported for categorical features and between consecutive spans of data\n",
    "  (i.e., between span N and span N+1), such as between different days of training data.\n",
    "    * L-infinity distance\n",
    "    * alerts when drift is higher than threshold distance\n",
    "* Skew\n",
    "    * data - schema skew\n",
    "      occurs when the training and serving data do not conform to the same schema.\n",
    "      Any expected deviations between the two (such as the label feature being only present in the training data but not in serving) should be specified through environments field in the schema.\n",
    "    * feature skew\n",
    "      occurs when the training and serving feature values are different\n",
    "      This can happen when:\n",
    "        * A data source of features is modified between training and serving time\n",
    "        * A difference in logic for generating between training and serving.\n",
    "    * distribution skew\n",
    "      occurs when the training and serving distributions are significantly different.\n",
    "      some key causes:\n",
    "        * different code or different data sources to generate the training dataset\n",
    "        * a faulty sampling mechanism that chooses a non-representative subsample of the serving data to train on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0770fd1-41d6-4f7a-848e-9e73589c10ca",
   "metadata": {},
   "source": [
    "### using TFDV to look at the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4c4dc0-6bbc-4e19-beba-ca8dc797ee2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tfx.dsl.io import fileio\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.components import StatisticsGen, SchemaGen, ExampleValidator\n",
    "from tfx.utils import io_utils\n",
    "from tensorflow_metadata.proto.v0 import anomalies_pb2, schema_pb2\n",
    "from tensorflow_metadata.proto.v0 import statistics_pb2\n",
    "from tensorflow_transform.tf_metadata import schema_utils\n",
    "import tensorflow_data_validation as tfdv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d3bcbb-7a5f-4732-9aea-8e491db909ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Schema from SchemaGen:\")\n",
    "_list = store.get_artifacts_by_type(\"Schema\")\n",
    "print(f'Schema count={len(_list)}')\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"SchemaGen\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "assert(artifact_uri is not None)\n",
    "file_path = os.path.join(artifact_uri, \"schema.pbtxt\")\n",
    "schema = tfdv.load_schema_text(file_path)\n",
    "#tfdv.visualize_artifacts(schema)\n",
    "tfdv.display_schema(schema=schema)\n",
    "\n",
    "print(\"ExampleStatistcs from StatisticsGen:\")\n",
    "_list = store.get_artifacts_by_type(\"ExampleStatistics\")\n",
    "print(f'ExampleStatistics count={len(_list)}')\n",
    "#print(f'ExampleStatistics ={_list}')\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"StatisticsGen\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "assert(artifact_uri is not None)\n",
    "file_paths = [os.path.join(artifact_uri, name, \"FeatureStats.pb\") \n",
    "  for name in os.listdir(artifact_uri)]\n",
    "stats_dict = {}\n",
    "for file_path in file_paths:\n",
    "    if \"test\" in file_path:\n",
    "        #plotting train and eval\n",
    "        continue\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized_stats = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")    \n",
    "    stats = statistics_pb2.DatasetFeatureStatisticsList()\n",
    "    stats.ParseFromString(serialized_stats)\n",
    "    if \"train\" in file_path:\n",
    "        stats_dict['train'] = stats\n",
    "    else:\n",
    "        stats_dict['eval'] = stats\n",
    "tfdv.visualize_statistics(lhs_statistics=stats_dict['eval'],\n",
    "    rhs_statistics=stats_dict['train'],\n",
    "    lhs_name='EVAL_DATASET', rhs_name='TRAIN_DATASET')\n",
    "\n",
    "print(\"ExampleAnomalies from ExampleValidator:\")\n",
    "_list = store.get_artifacts_by_type(\"ExampleAnomalies\")\n",
    "print(f'ExampleAnomalies count={len(_list)}')\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"ExampleValidator\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "assert(artifact_uri is not None)\n",
    "file_paths = [os.path.join(artifact_uri, name, \"SchemaDiff.pb\") \n",
    "  for name in os.listdir(artifact_uri)]\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized_anomalies = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Anomaly file not found at {file_path}\")\n",
    "    anomalies = anomalies_pb2.Anomalies()\n",
    "    anomalies.ParseFromString(serialized_anomalies)\n",
    "    tfdv.display_anomalies(anomalies)\n",
    "\n",
    "print(\"ExampleStatistics from pre-transform stats of Transform:\")\n",
    "_list = store.get_artifacts_by_type(\"ExampleStatistics\")\n",
    "#print(f'ExampleStatistics={_list}')\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"pre_transform_stats\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "assert(artifact_uri is not None)\n",
    "file_paths = [os.path.join(artifact_uri, name) \n",
    "  for name in os.listdir(artifact_uri)]\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized_stats = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")    \n",
    "    stats = statistics_pb2.DatasetFeatureStatisticsList()\n",
    "    stats.ParseFromString(serialized_stats)\n",
    "    tfdv.visualize_statistics(stats)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d681e-6788-456e-8a77-91c4ea809963",
   "metadata": {},
   "source": [
    "### using TFDV to look at the transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45664b-bfed-4877-a876-9663cc2e169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Schema from post-Transform:\")\n",
    "_list = store.get_artifacts_by_type(\"Schema\")\n",
    "print(f'Schema count={len(_list)}')\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"post_transform_schema\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "assert(artifact_uri is not None)\n",
    "file_path = os.path.join(artifact_uri, \"schema.pbtxt\")\n",
    "schema = tfdv.load_schema_text(file_path)\n",
    "#tfdv.visualize_artifacts(schema)\n",
    "tfdv.display_schema(schema=schema)\n",
    "\n",
    "print(\"ExampleStatistics from post-transform stats of Transform:\")\n",
    "_list = store.get_artifacts_by_type(\"ExampleStatistics\")\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"post_transform_stats\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "assert(artifact_uri is not None)\n",
    "file_paths = [os.path.join(artifact_uri, name) \n",
    "  for name in os.listdir(artifact_uri)]\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized_stats = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")    \n",
    "    stats = statistics_pb2.DatasetFeatureStatisticsList()\n",
    "    stats.ParseFromString(serialized_stats)\n",
    "    tfdv.visualize_statistics(stats)\n",
    "    \n",
    "print(\"ExampleAnomalies from post-transform of Transform:\")\n",
    "_list = store.get_artifacts_by_type(\"ExampleAnomalies\")\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"post_transform_anomalies\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "assert(artifact_uri is not None)\n",
    "file_paths = [os.path.join(artifact_uri, name) \n",
    "  for name in os.listdir(artifact_uri)]\n",
    "for file_path in file_paths:\n",
    "    anomalies = anomalies_pb2.Anomalies()\n",
    "    try:\n",
    "        with open(file_path, 'rb') as f:\n",
    "            serialized_anomalies = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Anomaly file not found at {file_path}\")\n",
    "    anomalies = anomalies_pb2.Anomalies()\n",
    "    anomalies.ParseFromString(serialized_anomalies)\n",
    "    tfdv.display_anomalies(anomalies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553960d4-7320-49b1-8cdd-dd5547a3057d",
   "metadata": {},
   "source": [
    "## Save the data schema with version control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd61195-54fd-4927-9d73-5e06dcaf52c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "_list = store.get_artifacts_by_type(\"Schema\")\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"pre_transform_schema\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "assert(artifact_uri is not None)\n",
    "pre_transform_file_path = [os.path.join(artifact_uri, name) for name in os.listdir(artifact_uri)][0]\n",
    "\n",
    "for artifact in _list:\n",
    "    if \"post_transform_schema\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "assert(artifact_uri is not None)\n",
    "post_transform_file_path = [os.path.join(artifact_uri, name) for name in os.listdir(artifact_uri)][0]\n",
    "\n",
    "pre_dir = os.path.join(get_project_dir(), \"src/main/resources\", \"pre_transform\")\n",
    "post_dir = os.path.join(get_project_dir(), \"src/main/resources\", \"post_transform\")\n",
    "os.makedirs(pre_dir, exist_ok=True)\n",
    "os.makedirs(post_dir, exist_ok=True)\n",
    "\n",
    "pre_schema_path = os.path.join(pre_dir, \"schema.pbtxt\")\n",
    "post_schema_path = os.path.join(post_dir, \"schema.pbtxt\")\n",
    "\n",
    "shutil.copyfile(pre_transform_file_path, pre_schema_path)\n",
    "shutil.copyfile(post_transform_file_path, post_schema_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307091b-211e-4f81-95ae-eed7ef9389b2",
   "metadata": {},
   "source": [
    "## Run baseline model pipeline with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d88cf-ffb3-4396-afcb-bc4c35ebac92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipeline_factory = PipelineComponentsFactory(\n",
    "  num_examples=1000209,\n",
    "  infiles_dict_ser=infiles_dict_ser, output_config_ser=output_config_ser,\n",
    "  transform_dir=tr_dir, user_id_max=user_id_max, movie_id_max=movie_id_max,\n",
    "  n_genres=n_genres, n_age_groups=n_age_groups, min_eval_size=MIN_EVAL_SIZE,\n",
    "  batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS, device=\"CPU\",\n",
    "  serving_model_dir=serving_model_dir,\n",
    ")\n",
    "\n",
    "beam_pipeline_args = [\n",
    "  '--direct_running_mode=multi_processing',\n",
    "  '--direct_num_workers=0']\n",
    "\n",
    "baseline_components = pipeline_factory.build_components(PIPELINE_TYPE.BASELINE,\n",
    "  run_example_diff=False, pre_transform_schema_dir_path=pre_dir,\n",
    "  post_transform_schema_dir_path=post_dir)\n",
    "\n",
    "# create baseline model\n",
    "my_pipeline = tfx.dsl.Pipeline(\n",
    "  pipeline_name=PIPELINE_NAME,\n",
    "  pipeline_root=PIPELINE_ROOT,\n",
    "  components=baseline_components,\n",
    "  enable_cache=ENABLE_CACHE,\n",
    "  metadata_connection_config=metadata_connection_config,\n",
    "  beam_pipeline_args=beam_pipeline_args,\n",
    ")\n",
    "\n",
    "tfx.orchestration.LocalDagRunner().run(my_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb51e922-705c-4127-9655-bad02fb27fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'plot ModelRun using tensorboard')\n",
    "%load_ext tensorboard\n",
    "_list = store.get_artifacts_by_type(\"ModelRun\")\n",
    "print(f'ModelRun count={len(_list)}')\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"Trainer\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        break\n",
    "%tensorboard --logdir {artifact_uri}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe0a7d9-5203-4698-9de8-a51d7811856a",
   "metadata": {},
   "source": [
    "## Run full model pipeline with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b85116-6266-4777-9444-f938f174a25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifact_types = store.get_artifact_types()\n",
    "logging.debug(f\"MLMD store artifact_types={artifact_types}\")\n",
    "artifacts = store.get_artifacts()\n",
    "logging.debug(f\"MLMD store artifacts={artifacts}\")\n",
    "\n",
    "components = pipeline_factory.build_components(PIPELINE_TYPE.PRODUCTION,\n",
    "  run_example_diff=False, pre_transform_schema_dir_path=pre_dir,\n",
    "  post_transform_schema_dir_path=post_dir)\n",
    "  \n",
    "# simulate experimentation of one model family\n",
    "my_pipeline = tfx.dsl.Pipeline(\n",
    "  pipeline_name=PIPELINE_NAME,\n",
    "  pipeline_root=PIPELINE_ROOT,\n",
    "  components=components,\n",
    "  enable_cache=ENABLE_CACHE,\n",
    "  metadata_connection_config=metadata_connection_config,\n",
    "  beam_pipeline_args=beam_pipeline_args,\n",
    ")\n",
    "\n",
    "tfx.orchestration.LocalDagRunner().run(my_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820d838-34a3-4860-bf1a-5e23ba2af06a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add training and eval plots: Trainer/model_run\n",
    "#  Tensorboard can visualize them\n",
    "print(f'plot ModelRun using tensorboard')\n",
    "%load_ext tensorboard\n",
    "_list = store.get_artifacts_by_type(\"ModelRun\")\n",
    "print(f'ModelRun count={len(_list)}')\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "for artifact in _list:\n",
    "    if \"Trainer\" in artifact.uri:\n",
    "        artifact_uri = artifact.uri\n",
    "        %tensorboard --logdir {artifact_uri}\n",
    "\n",
    "print(\"ModelEvaluation from Evaluator:\")\n",
    "_list = store.get_artifacts_by_type(\"ModelEvaluation\")\n",
    "print(f'Schema count={len(_list)}')\n",
    "_list = sorted(_list, key=lambda x: x.create_time_since_epoch, reverse=True)\n",
    "artifact_uri = _list[0].uri\n",
    "assert(artifact_uri is not None)\n",
    "#file_path = os.path.join(artifact_uri, \"metrics*\")\n",
    "eval_result = tfma.load_eval_result(artifact.uri)\n",
    "tfma.view.render_slicing_metrics(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406cccb0-6a62-4344-a281-8e63f56cd1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
