libraries:
  - apache-beam
  - TFX
  - TensorFlow Data Validation
  - TensorFlow Transform
  - tfx_bsl

need to use a venv because of the earlier version of python

TFX is not available for all architectures.  e.g. apple arm m4 pro.

TFX is compatible with a restricted range of python and tensorflow.
see https://pypi.org/project/tfx/
use python 3.9 or 3.10

considering the various ways to pre-process, transform data.
wanting a component for transformations in a pipeline to automate the process,
and want the transformations to be distributable.
- tf.data is for in-memory size data processing so not an option here.
- consider using keras layers
  pros:
    - less code, simpler to maintain
  cons:
    - by performing the transformation to featurees in the model, we are
      repeating work, especially during the compute intensive hyper-parameter
      tuning phase.
    - features are not saved in a feature store, which might be a preference
      for some organization
    - cannot perform stats on features
- using tft and tfx
  pros:
    - transform to features once and that is input to the hyper-parameter tuning
  cons:
    - it's more work, at least while I'm ramping up.

place the code for recommender systems into TFX components to make a 
pipeline which is runnable by a local spark runner.

considering custom python functions and using the TFX dsl annotations.

TFXIO might be useful for transforming data into tfrecords.

tf.Transform methods that make a pass through all of the data are needed 
to count records and to define vocabularies, else the operations are 
parallelizable in pre-processing..

consider writing a pipeline with iteration, placeholder for approval, and
placeholder for deployment.

would like to use the custom python functions with dsl annotations

also create the compiler statements needed to create the yaml intermediate file
for running with airflow or kfp etc

The transformations can either occur in the pipeline before training,
or can occur as keras layers within the model.
An advantage to preprocessing in pipeline before training, is that it makes
it easier to reuse the code in a pipeline that makes curated features for a
feature store.  It also makes it easier to use EDA on the features in contrast to
using keras layers in transformations to features.
Note that the nomenclature is that raw data is transformed to features and
the features are the direct inputs to the models.

- there will be scripts to set up the environment (provision a workbench).
  including setup of scan or other vector db
- there will be python function custom components
  - ingest, validate data, transform, train, tune, evaluate
- there will be a custom component to use the trained model to reconstruct
  the tf/keras model and extract the query and candidate models.
- there will be a custom component to create movie vector embeddings
  and store them in the vector db.
- there will be an application which:
  - downloads data if doesn't exist
  - runs pipeline
- the evaluate function:
  - receives user or query information,
  - transforms the data
  - performs a db lookup to find candidate movies


